import os
import pandas as pd
import numpy as np
import string, os
from tensorflow import keras
from keras.utils import pad_sequences
from keras.layers import Embedding, Dropout, Bidirectional, LSTM, Dense
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from IPython.display import FileLink
from keras.models import load_model
import sys


# data prep (move to separate file)
data = pd.read_csv('lyrics-data1.csv')

drop_features = ['ALink', 'SName', 'SLink']
data.drop(drop_features, axis = 1, inplace = True)

# Shouldn't this filter it to be only English? 
data_en = data[data['language'] == 'en'] 

# english is about rows 335 to 69920
# average song has like 400 words?
# ideal model size would be 1-2 mil for full data set in English
data_en = data_en[:2000]

#print(data_en)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(data_en['Lyric'])
tokenized_sentences = tokenizer.texts_to_sequences(data_en['Lyric'])

#word_index = tokenizer.word_index

max_seq_len = 20
X = []
y = []
for seq in tokenized_sentences:
    for i in range(1, len(seq)):
        X.append(seq[max(0, i - max_seq_len):i])
        y.append(seq[i])

X = pad_sequences(X, maxlen = 20, padding = 'pre')

y_array = np.array(y)

# file break here?

# these are supposedly all arrays huh
X_train, X_test, Y_train, Y_test = train_test_split(X, y_array, test_size = 0.2, random_state = 25)

# the generated title would also feed into here 

model = keras.Sequential()
total_words = len(tokenizer.word_index) + 1
model.add(Embedding(input_dim = total_words, output_dim = 64))
model.add(Bidirectional(LSTM(150)))
model.add(Dropout(0.1)) # is literally losing brain cells 
model.add(Dense(15))
model.add(Dense(total_words, activation = 'softmax'))

#print(model.summary())

#model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

#model.save('model.h5')

# file break

model = load_model('model.h5')

def complete_this_song(seed_text, next_words):

    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')
        
        predicted_probs = model.predict(token_list, verbose=0)
        predicted = np.argmax(predicted_probs, axis=-1)
        
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += " " + output_word
    print(seed_text)


complete_this_song(sys.argv[1], int(sys.argv[2]))




# Ways to improve the lyric generator

# NN on this file inputs the topic or the title generated by the first one (I could try either way and 
# see what works better)
# it would also have to learn what words are closely related to the song topic, find the most common 
# first word the goes off of that
    # How should it determine song length?
# it would also have to treat new lines as "words" (such as adding a new line if it's statistically 
# the most likely next thing)
# or i could manually handle that (if next word is a space or whatever it would be, print new line)
# i should also add a word cap of like 300-400 words
# prints whole thing out at the end 


# i identify as a computer psychologist :))